{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TD2 : image classification",
   "id": "56310144af85a1ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "S. Delprat, INSA Hauts-de-France  \n",
    "  ![Logo_insa_hauts_de_france.jpg](data:image/jpeg;base64,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)\n"
   ],
   "id": "e3504f103494597f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a guided exercice to understand how to build an efficient classification network. The dataset has been kept small enough such that the training time remain small.",
   "id": "a3f5d5ec9660c818"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Necessary on collab\n",
    "# %pip install torchinfo"
   ],
   "id": "a87d815b5a8dc8ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import zipfile\n",
    "import time\n",
    "import random\n",
    "\n",
    "# MUST BE RUN BEFORE 'import torch' (for reproducibility, see below)\n",
    "# We must ensure CUBLAS is also deterministic, or it will throw an error.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "# Data related\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)                           # For Python\n",
    "np.random.seed(SEED)                        # For Numpy, or modules that uses numpy\n",
    "torch.manual_seed(SEED)                     # For PyTorch\n",
    "torch.cuda.manual_seed_all(SEED)            # Sets the seed for CPU and GPU\n",
    "torch.backends.cudnn.benchmark = False      # Enforces CUDA to always use the same algorithms. May decrease performances.\n",
    "torch.backends.cudnn.deterministic = True   # Enforces the given algorithms to be deterministic."
   ],
   "id": "1dcfd7f42a8447bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check cuda version",
   "id": "25be24623417d573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.__version__",
   "id": "111c4fd5cf85f351"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Configuration of the hyperparameters",
   "id": "d7196ced94d0c398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experimentName      = \"step2\"\n",
    "datasetFolder       = '../data/processed/datasetFlower'\n",
    "outputFolder        = os.path.join(\"../models/td2/result\", experimentName)\n",
    "batchSize           = 32        # Keep it low for faster iterations (for low-end gpus)\n",
    "n_channels          = 3         # Colored images\n",
    "H, W                = 64, 64    # image height & width\n",
    "learningRate        = 0.01      # Size of the step to make to optimize weights at each iteration.\n",
    "validateEveryIter   = 10        # How many iters before validation\n",
    "numEpoch            = 100       # Number of epoch for the training\n",
    "weight_decay        = 1e-4      # Regularize weights to prevent exploding gradient and overfitting."
   ],
   "id": "9a66bca06e8818f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Empty & create output folder",
   "id": "a0ea36dba0ec8056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if os.path.isdir(outputFolder):\n",
    "    shutil.rmtree(outputFolder)\n",
    "os.makedirs(outputFolder)"
   ],
   "id": "5eaf939a66ad7ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q1.** Check for GPU\n",
    "Create a variable `device` that is equal to `cuda` or `cpu` depending on the availability of a GPU"
   ],
   "id": "d417c2393f0af522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for GPU.\n",
    "for gpu_idx in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {gpu_idx}: {torch.cuda.get_device_name(gpu_idx)}\")\n",
    "\n",
    "# Create a variable 'device' that is equal to 'cuda' if a GPU is available, and 'cpu' if no GPU is available.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)"
   ],
   "id": "a41a64687943e182"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1) Working with the studied dataset",
   "id": "5cbb3070b9295427"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2) Load some data",
   "id": "e6a3deff29a28312"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q2.** Import data\n",
    "1. Copy the \"flower\" zip file into your Google Drive.\n",
    "2. Share it using link\n",
    "3. Use the file ID to copy this file into this Colab virtual machine (using gdown).\n"
   ],
   "id": "a0c82f6384949ea2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We run locally here, so we just put it at the correct path.",
   "id": "c2d9b3204b00a89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if the flower.zip file is in the current directory\n",
    "# If not, download it from the given URL\n",
    "# If yes, print \"File already exists\"\n",
    "archiveName = \"../data/raw/flower.zip\"\n",
    "if not os.path.isfile(archiveName):\n",
    "    print(\"Download the file and place it in the current directory\")\n",
    "else:\n",
    "    print(f\"File {archiveName} already in the current directory\")"
   ],
   "id": "b831fda878770131"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract the archive into the `datasetFolder`",
   "id": "1560db22c1f1b0ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not os.path.isdir(datasetFolder):\n",
    "    # uncompress the archive\n",
    "    print(datasetFolder)\n",
    "    with zipfile.ZipFile(os.path.join(os.getcwd(), archiveName), 'r') as zip_ref:\n",
    "        zip_ref.extractall(datasetFolder)\n",
    "        print(\"Dataset is extracted\")\n",
    "else:\n",
    "    print(\"Dataset is already extracted\")"
   ],
   "id": "80b6cd92af93cb59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2) Dataset & dataloader\n",
    "\n"
   ],
   "id": "5334a5cd1cea5e72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following object allows loading the entire dataset into memory. As a result, there is no need to load images on the fly, and the CPU computation requirement is less.\n",
    "However, the machine needs to have enough memory to store all the data."
   ],
   "id": "4e8cebc2a9926138"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class inMemoryImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, imSize=None, NbClass=None, maxFilesPerClass=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.imSize = imSize\n",
    "        self.classes = sorted(entry.name for entry in os.scandir(self.root) if entry.is_dir())\n",
    "\n",
    "        if not self.classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {self.root}.\")\n",
    "\n",
    "        if NbClass is not None:\n",
    "            # Limit the number of class\n",
    "            self.classes = self.classes[:NbClass]\n",
    "\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Scan each class folder\n",
    "        self.allFilename = []\n",
    "        self.allIdx = []\n",
    "\n",
    "        for i, className in enumerate(self.classes):\n",
    "            idx = self.class_to_idx[className]\n",
    "            target_dir = os.path.join(self.root, className)\n",
    "            classFileNameList = []\n",
    "            classIdxList = []\n",
    "            # sort all the subfolders\n",
    "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "                for file in fnames:\n",
    "                    if maxFilesPerClass is None or len(classFileNameList) < maxFilesPerClass:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        classFileNameList.append(file_path)\n",
    "                        classIdxList.append(idx)\n",
    "            self.allFilename = self.allFilename + classFileNameList\n",
    "            self.allIdx = self.allIdx + classIdxList\n",
    "\n",
    "        # read images\n",
    "        self.allImgs = []\n",
    "        progressBar = tqdm(\n",
    "            self.allFilename,\n",
    "            desc=\"Loading dataset in memory\",\n",
    "            total=len(self.allFilename))\n",
    "\n",
    "        for file in progressBar:\n",
    "            with open(file, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                if self.imSize is not None:\n",
    "                    self.allImgs.append(img.resize(self.imSize).convert(\"RGB\"))\n",
    "                else:\n",
    "                    self.allImgs.append(img.convert(\"RGB\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.allFilename)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.allImgs[idx], self.allIdx[idx]"
   ],
   "id": "13a9caf24d9b99da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q3.** Dataset creation\n",
    "* Create a dataset `all_train_data` for the training data, with no transformations.\n",
    "* Split this dataset into `train_data` (70%) and `validation_data` (30%).\n",
    "* Create a dataset `test_data` with the test images\n",
    "* Create a variable `nClass` containing the number of classes from the dataset.\n",
    "* Store the available classes in `classes`\n",
    "* Store the available classes in `classes`\n",
    "\n",
    "NB: you may/should use the `inMemoryImageFolder` instead of the original `ImageFolder` dataset provided by PyTorch.\n",
    "\n"
   ],
   "id": "aed1ab3ef672341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# * Create a dataset `all_train_data` for the training data, with no transformations. Train data are located in the `train` folder.\n",
    "# * Split this dataset into `train_data` (70%) and `validation_data` (30%).\n",
    "# * Create a dataset `test_data` with the test images. Test data are located in the `test` folder.\n",
    "# * Create a variable `nClass` containing the number of classes from the dataset.\n",
    "# * Store the available classes in `classes`\n",
    "# * Store the available classes in `classes`\n",
    "\n",
    "# NB: you may/should use the `inMemoryImageFolder` instead of the original `ImageFolder` dataset provided by PyTorch.\n",
    "\n",
    "print(\"Previous version that doesn't stratify using the classes\")\n",
    "# all_train_data              = inMemoryImageFolder(os.path.join(datasetFolder, \"train\"), imSize=(H, W))\n",
    "# train_data, validation_data = torch.utils.data.random_split(all_train_data, [0.7, 0.3])\n",
    "# test_data                   = inMemoryImageFolder(os.path.join(datasetFolder, \"test\"), imSize=(H, W))\n",
    "# nClass                      = len(all_train_data.classes)\n",
    "# classes                     = all_train_data.classes"
   ],
   "id": "9918d3014014e083"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We want to stratify so that the class distribution is similar between the train, test and validation.",
   "id": "bba588ae25beb767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Load the entire dataset\n",
    "all_train_data = inMemoryImageFolder(\n",
    "    os.path.join(datasetFolder, \"train\"),\n",
    "    imSize=(H, W)\n",
    ")\n",
    "\n",
    "# 2) Prepare for stratified splitting:\n",
    "#    -> We'll need an index list [0, 1, 2, ...] and a labels list for stratification\n",
    "all_indices = list(range(len(all_train_data)))\n",
    "all_labels = [all_train_data.allIdx[i] for i in all_indices]  # numeric labels of every sample\n",
    "\n",
    "# 3) We perform the stratified split (70% train / 30% validation here)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.3,\n",
    "    stratify=all_labels, # Equal class distribution\n",
    "    random_state=SEED    # Reproducibility\n",
    ")\n",
    "\n",
    "# 4) We create `Subset` objects for PyTorch\n",
    "train_data = Subset(all_train_data, train_idx)\n",
    "validation_data = Subset(all_train_data, val_idx)\n",
    "\n",
    "# 5) Load separate test data folder (no split needed)\n",
    "test_data = inMemoryImageFolder(\n",
    "    os.path.join(datasetFolder, \"test\"),\n",
    "    imSize=(H, W)\n",
    ")\n",
    "\n",
    "# 6) Access classes and define variables\n",
    "nClass = len(all_train_data.classes)\n",
    "classes = all_train_data.classes\n",
    "\n",
    "print(f\"Training samples: {len(train_data)} | Validation samples: {len(validation_data)} | Test samples: {len(test_data)}\")"
   ],
   "id": "cb8fd5700026bd8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Number of classes: {nClass}\\nClasses: \", end=\"\")\n",
    "for data_class in classes[:-1]:\n",
    "    print(data_class, end=\", \")\n",
    "print(classes[-1])"
   ],
   "id": "7a53df4b600aa6ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We verify that the class distribution is similar",
   "id": "5150d391a4e58983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Gather labels from splits\n",
    "# Extract numeric labels (indices) from train_data\n",
    "train_labels = [all_train_data.allIdx[i] for i in train_data.indices]\n",
    "\n",
    "# Extract numeric labels from validation_data\n",
    "val_labels = [all_train_data.allIdx[i] for i in validation_data.indices]\n",
    "\n",
    "# Optionally, if you want to include test_data as well:\n",
    "test_labels = [test_data.allIdx[i] for i in range(len(test_data))]\n",
    "\n",
    "# 2) Count how many samples per class\n",
    "train_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "val_counts   = pd.Series(val_labels).value_counts().sort_index()\n",
    "test_counts  = pd.Series(test_labels).value_counts().sort_index()\n",
    "\n",
    "# 3) Build a single DataFrame combining all splits\n",
    "df_train = pd.DataFrame({\n",
    "    'class_idx': train_counts.index,\n",
    "    'count': train_counts.values,\n",
    "    'split': 'train'\n",
    "})\n",
    "df_val = pd.DataFrame({\n",
    "    'class_idx': val_counts.index,\n",
    "    'count': val_counts.values,\n",
    "    'split': 'validation'\n",
    "})\n",
    "df_test = pd.DataFrame({\n",
    "    'class_idx': test_counts.index,\n",
    "    'count': test_counts.values,\n",
    "    'split': 'test'\n",
    "})\n",
    "\n",
    "df = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "# Convert the numeric class_idx to class names (optional, but more readable)\n",
    "df['class_name'] = df['class_idx'].apply(lambda idx: all_train_data.classes[idx])\n",
    "\n",
    "# 4) Plot with seaborn\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df, x='class_name', y='count', hue='split')\n",
    "plt.title(\"Class Distribution in Train / Validation / Test Splits\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "593a5947161a848"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q4.** Create required transformations:\n",
    "* `formatTransform`  : comprising `Resize`, `PILToTensor`, `ToDtype`\n",
    "* `augmentTransform` : `RandomRsizedCrop`, `RandomHorizontalFlip`, `PILToTensor`, `ToDtype`\n",
    "\n",
    "Nb: the images should be `float32` tensor. Use `v2.ToDtype(torch.float32, scale=True)`"
   ],
   "id": "b7f77b11439f708e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create required transformations:\n",
    "# * `formatTransform`  : comprising `Resize`, `PILToTensor`, `ToDtype`\n",
    "# * `augmentTransform` : `RandomResizedCrop`, `RandomHorizontalFlip`, `PILToTensor`, `ToDtype`\n",
    "\n",
    "# Nb: the images should be `float32` tensor. Use `v2.ToDtype(torch.float32, scale=True)`\n",
    "\n",
    "formatTransform = v2.Compose([\n",
    "    v2.Resize((H, W)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "augmentTransform = v2.Compose([\n",
    "    v2.RandomResizedCrop((H, W)),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ],
   "id": "10bef3e9ba875ea0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `transformedDataset` object, derived from `the torch.utils.data.Dataset` class, applies a given transformation to an existing dataset.",
   "id": "5fd96e3e534e9302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class transformedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        return self.transforms(image), label  # Get the image, not the sample"
   ],
   "id": "1a6e3960dc04080a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q5.** Create 3 `transformedDataset` objects (`train_data`, `validation_data`,`test_data`), one for each subset (train, validation and test) using the `formatTransform` transformation defined in question Q5 (`augmentTransform` will only be used later)",
   "id": "85e8ac71281b45f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# **Q5.** Create 3 `transformedDataset` objects (`train_data`, `validation_data`,`test_data`), one for each subset\n",
    "# (train, validation and test) using the `formatTransform` transformation defined in question Q5\n",
    "# (`augmentTransform` will only be used later)\n",
    "\n",
    "# Previous step\n",
    "# train_data = transformedDataset(train_data, formatTransform)\n",
    "# validation_data = transformedDataset(validation_data, formatTransform)\n",
    "# test_data = transformedDataset(test_data, formatTransform)\n",
    "\n",
    "# We now augment the date\n",
    "train_data      = transformedDataset(train_data, augmentTransform)\n",
    "validation_data = transformedDataset(validation_data, augmentTransform)\n",
    "test_data       = transformedDataset(test_data, augmentTransform)"
   ],
   "id": "d85169fab3d8e3a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q7.** define a dataloader for all the datasets",
   "id": "c04e391110fbfc0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Define a dataloader for all the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batchSize, shuffle=True, generator=g, worker_init_fn=seed_worker)\n",
    "val_loader = torch.utils.data.DataLoader(validation_data, batch_size=batchSize, shuffle=True, generator=g, worker_init_fn=seed_worker)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batchSize, shuffle=False)"
   ],
   "id": "cad321bd39eee47e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the dataloader to retrieve one batch and display some images",
   "id": "57f7d13370b3d1df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "imgsBatch, lblBatch = next(iter(train_loader))\n",
    "nImgs = 30\n",
    "cols = int(math.ceil(math.sqrt(nImgs)))  # Number of columns (square root rounded up)\n",
    "rows = int(math.ceil(nImgs / cols))  # Number of rows (ensure enough rows to fit images)\n",
    "\n",
    "# Create the figure with the calculated number of rows and columns\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))  # Adjust figsize to the grid\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(nImgs):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(imgsBatch[i, :].permute(1, 2, 0))\n",
    "    ax.set_title(classes[lblBatch[i]], fontsize=10)\n",
    "    ax.axis('off')"
   ],
   "id": "8ec9e861234a2df7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2) Define a model",
   "id": "3066a146a9dd2ea2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is where the magic happens.",
   "id": "ac2c6a393e4fe08d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q6.** Define the `myConvNet` model class as described in the subject. The model takes a batch of images at input and provides a batch of logits as an output.",
   "id": "549992699d4f7c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class myConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myConvNet, self).__init__()\n",
    "\n",
    "        # 1) First conv (3 -> 8), ReLU, then AvgPool 22\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 2) Second conv (8 -> 16), ReLU, then AvgPool 22\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3) Third conv (16 -> 32), ReLU, then AvgPool 22\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 4) Adaptive average pooling to 44\n",
    "        self.adaptivePool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        # 5) Final fully connected layer (32*4*4 -> nClass)\n",
    "        self.fc = nn.Linear(32 * 4 * 4, nClass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        # block 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        # block 3\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        # adaptive pooling + flatten\n",
    "        x = self.adaptivePool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ],
   "id": "fc4222dd6d373127"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q7.** Create an instance of your model object `model` and send it to the GPU if available (use the \"device\" variable defined in Q1)",
   "id": "39d7ba233370a24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = myConvNet().to(device)",
   "id": "2f6d128806695306"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q8.** Display model using `summary` from the `torchsummary` module.",
   "id": "98cbb519661f3d61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display model using summary from the torchsummary module\n",
    "summary(model, input_size=(batchSize, n_channels, H, W))"
   ],
   "id": "c5ca924fcc8ab882"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3) Train network",
   "id": "440c0f408792bb38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1) define optimizer",
   "id": "14d75a5c8b8f8397"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q9.** Define an Adam optimizer for the model parameters with:\n",
    "* `learningRate`: learning rate\n",
    "* `weight_decay`: L2 weigh parameter\n",
    "\n"
   ],
   "id": "ffbe6967d2671e9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define an Adam optimizer for the model parameters with:\n",
    "# * learning rate `learningRate`\n",
    "# * L2 weight parameter `weight_decay`\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weight_decay)"
   ],
   "id": "cd1f6793a00479f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q10.** Define criterion object (`torch.nn.CrossEntropyLoss`) with `sum` as a reduction scheme. Do not forget to send this object to the computing device\n",
    "\n",
    "\n",
    "Be aware that the CrossEntropyLoss function inputs is logits\n",
    "* you must NOT include the `softmax` function in the network\n",
    "* `softmax` function is integrated within the `CrossEntropyLoss` function\n",
    "\n",
    "You must send the function to the selected device (CPU or GPU) according to the `device` variable"
   ],
   "id": "d6b95af02a055bbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define criterion object (`torch.nn.CrossEntropyLoss`) with `sum` as a reduction scheme. Do not forget to send this object to the computing device\n",
    "\n",
    "\n",
    "# Be aware that the CrossEntropyLoss function inputs is logits\n",
    "# * you must NOT include the `softmax` function in the network\n",
    "# * `softmax` function is integrated within the `CrossEntropyLoss` function\n",
    "\n",
    "# You must send the function to the selected device (CPU or GPU) according to the `device` variable\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum').to(device)"
   ],
   "id": "6841e5daf0cc57ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2) Setup tensorboard for visualization",
   "id": "c8f5bd51b7f55ade"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tensorboard should only be setup once the main program is working. For this exercise, priority is set on writing a working code, visualization comes after",
   "id": "ab1171117d8dffcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q11.** Load the Tensorboard extension and eventually empty the log folder (using a command such as !rm -rf ./logs/)",
   "id": "92c861b72b65af77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#TensorBoard: cleanup to be used if you are using windows\n",
    "tensorBoardFolder = \"../models/td2/logs\"\n",
    "if os.path.isdir(tensorBoardFolder):\n",
    "    shutil.rmtree(tensorBoardFolder)"
   ],
   "id": "75a3619b2acd024e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ],
   "id": "87cb10ad71658087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q12.** Create 2 `torch.utils.tensorboard.SummaryWriter` object called `writerTrain` and `writerVal` pointing to `./logs/train` and `./logs/val`",
   "id": "a997283f5616a4ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "writerTrain = SummaryWriter(os.path.join(tensorBoardFolder, \"train\"))\n",
    "writerVal = SummaryWriter(os.path.join(tensorBoardFolder, \"val\"))"
   ],
   "id": "be264b59f9d642dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following line load the tensorboard extension",
   "id": "e72960aff343bad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%tensorboard --logdir \"../models/td2/logs\"",
   "id": "8085010b8150e871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q13.** load a batch of data from `train_loader`. Send the images to the device and use the `writerTrain.add_graph` method to allow the model visualization from tensorboard",
   "id": "15d4a16c4ff369dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load a batch of data from `train_loader`. Send the images to the device and use the `writerTrain.add_graph` method to \n",
    "# allow the model visualization from tensorboard\n",
    "\n",
    "imgsBatch, lblBatch = next(iter(train_loader))\n",
    "imgsBatch = imgsBatch.to(device)\n",
    "writerTrain.add_graph(model, imgsBatch)"
   ],
   "id": "84bbe04f57612372"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3) Main training loop",
   "id": "cbc6414fbe1863c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Q14.** Create a python dictionary `history` with the following keys : `train_loss`,`train_acc`,`train_x`,`val_loss`,`val_acc`,`val_x`. Each of these keys are associated with an empty list.",
   "id": "20612146aab8dbff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history = {\n",
    "    \"train_loss\": [],   # Training Loss\n",
    "    \"train_acc\": [],    # Training accuracy\n",
    "    \"train_x\": [],      # We accumulate the number of iterations of training data for plotting\n",
    "    \"val_loss\": [],     # Validations Loss\n",
    "    \"val_acc\": [],      # Validation accuracy\n",
    "    \"val_x\": []         # We accumulate the number of iterations of training data for plotting\n",
    "}"
   ],
   "id": "fbe6526b67c0c22e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.3) Illustration of the tqdm progress bar (to be used in the main loop)\n",
    "\n",
    "tqdm allows displaying a progress bar. It can be used within a loop to enumerate an iterable (a simple list in this case, but it can be a dataloader object as well). You need to provide (beside the iterable object) the number of iterations in the \"total\" parameter, a description in the \"desc\" parameter.\n",
    "\n",
    "At every iteration, you may write some values at the end of the line by providing a dicrtionary to the \"set_postfix\" method\n"
   ],
   "id": "e1f6870b4a42d6e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "progress_bar_ex = tqdm(\n",
    "    enumerate([10, 20, 30, 40]),\n",
    "    desc=\"Begining of the line \",\n",
    "    total=4)\n",
    "\n",
    "j_iter = 0\n",
    "for i_iter in progress_bar_ex:\n",
    "    j_iter = j_iter + j_iter\n",
    "    time.sleep(0.3)\n",
    "    progress_bar_ex.set_postfix(\n",
    "        {\n",
    "            \"curr j val\": j_iter,\n",
    "            \"curr i val\": i_iter\n",
    "        })"
   ],
   "id": "ddd58ddba43c2403"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q15.** Write the main loop code\n",
    "\n",
    "Read carefully all the instructions provided in the subject.\n"
   ],
   "id": "8ce4c8f293792ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(num_epochs=numEpoch):\n",
    "    \"\"\"\n",
    "    Train the model for `num_epochs` epochs,\n",
    "    validating every `validateEveryIter` iterations,\n",
    "    but keep only a single progress bar across epochs.\n",
    "    \"\"\"\n",
    "    print(f\"Training using {device}\")\n",
    "    start = time.time()\n",
    "    iteration = 0\n",
    "\n",
    "    # For tracking last validation predictions and labels\n",
    "    val_loss, val_acc = None, None\n",
    "\n",
    "    # Calculate total training steps (epochs * number_of_batches)\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "    # Create a single progress bar for all epochs\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Training\", leave=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for imgsBatch, yTrue in train_loader:\n",
    "            iteration += 1\n",
    "\n",
    "            # Forward & Backward passes\n",
    "            imgsBatch, yTrue = imgsBatch.to(device), yTrue.to(device)   # Send to device\n",
    "            optimizer.zero_grad()                                       # Reset Gradients every iter\n",
    "            yPred = model(imgsBatch)                                    # Infer\n",
    "            loss = criterion(yPred, yTrue)                              # Compute loss\n",
    "            loss.backward()                                             # Compute gradients and back-propagate it\n",
    "            optimizer.step()                                            # Take a step in the best direction\n",
    "\n",
    "            # Normalize loss and accuracy by batch size\n",
    "            batch_size = imgsBatch.size(0)\n",
    "            train_loss = loss.item() / batch_size\n",
    "            train_acc = (yPred.argmax(dim=1) == yTrue).sum().item() / batch_size\n",
    "\n",
    "            # Store training metrics in history\n",
    "            history[\"train_x\"].append(iteration)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "            # Write to TensorBoard\n",
    "            writerTrain.add_scalar(\"Loss\", train_loss, iteration)\n",
    "            writerTrain.add_scalar(\"Accuracy\", train_acc, iteration)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)  # Advance by one batch\n",
    "            progress_bar.set_description_str(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            progress_bar.set_postfix({\n",
    "                \"Training Loss\": f\"{train_loss:.3f}\",\n",
    "                \"Training Accuracy\": f\"{train_acc:.3f}\",\n",
    "                \"Validation Loss\": f\"{val_loss:.3f}\" if val_loss is not None else None,\n",
    "                \"Validation Accuracy\": f\"{val_acc:.3f}\" if val_acc is not None else None,\n",
    "            })\n",
    "\n",
    "            # Validate every validateEveryIter (such as every 10 iterations)\n",
    "            if iteration % validateEveryIter == 0:\n",
    "                val_loss, val_acc = validate_once(\n",
    "                    model, val_loader, criterion, history, writerVal, iteration, device\n",
    "                )\n",
    "\n",
    "    # Close the progress bar after all epochs\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Compute the elapsed time\n",
    "    train_duration = time.time() - start\n",
    "    print(f\"Training duration: {train_duration:.2f}s\")\n",
    "    return train_duration\n",
    "\n",
    "\n",
    "def validate_once(model, val_loader, criterion, history, writerVal, iteration, device):\n",
    "    \"\"\"\n",
    "    Run validation on the entire validation set.\n",
    "    Returns validation loss, accuracy, predictions, and labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_x = 0\n",
    "\n",
    "    yPreds, yTrues = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgsBatch, yTrue in val_loader:\n",
    "            imgsBatch, yTrue = imgsBatch.to(device), yTrue.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            yPred = model(imgsBatch)\n",
    "            loss = criterion(yPred, yTrue)\n",
    "\n",
    "            batch_size = imgsBatch.size(0)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (yPred.argmax(dim=1) == yTrue).sum().item()\n",
    "            val_x += batch_size\n",
    "\n",
    "            yPreds.append(yPred.argmax(dim=1).cpu().numpy())\n",
    "            yTrues.append(yTrue.cpu().numpy())\n",
    "\n",
    "    val_loss /= val_x\n",
    "    val_acc /= val_x\n",
    "\n",
    "    # Concatenate all predictions and ground truths\n",
    "    yPreds = np.concatenate(yPreds)\n",
    "    yTrues = np.concatenate(yTrues)\n",
    "\n",
    "    # Record validation metrics\n",
    "    history[\"val_x\"].append(iteration)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    # Write to TensorBoard\n",
    "    writerVal.add_scalar(\"Loss\", val_loss, iteration)\n",
    "    writerVal.add_scalar(\"Accuracy\", val_acc, iteration)\n",
    "\n",
    "    # Compute Confusion Matrix using the last validation predictions\n",
    "    cm = confusion_matrix(yTrues, yPreds)\n",
    "    df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    writerVal.add_figure(\"Confusion Matrix\", plt.gcf(), iteration)\n",
    "\n",
    "    return val_loss, val_acc"
   ],
   "id": "de4d2a593a654639"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainingDuration = train(num_epochs=numEpoch)",
   "id": "297475f5dfa2d233"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.4) Save model and training curve data",
   "id": "f2d4f83a9cd04563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model, os.path.join(outputFolder, \"model.zip\"))",
   "id": "52ad81e7f503347d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create separate DataFrames for train and validation using iteration as the key\n",
    "df_train = pd.DataFrame({\n",
    "    \"Iteration\": history['train_x'],\n",
    "    \"Training Loss\": history['train_loss'],\n",
    "    \"Training Accuracy\": history['train_acc']\n",
    "})\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    \"Iteration\": history['val_x'],\n",
    "    \"Validation Loss\": history['val_loss'],\n",
    "    \"Validation Accuracy\": history['val_acc']\n",
    "})\n",
    "\n",
    "# Merge on iteration\n",
    "df_combined = pd.merge(df_train, df_val, on=\"Iteration\", how=\"outer\")\n",
    "df_combined.to_csv(os.path.join(outputFolder, \"metrics_data.csv\"), sep=';', index=False)"
   ],
   "id": "6c5d199808a57460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Shape the combined dataframe: {df_combined.shape}\")\n",
    "df_combined.head(100)"
   ],
   "id": "c8f40935cb71b604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4) Analyze obtained results",
   "id": "dcea8bf25238d34d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1) Training curves",
   "id": "a0ce229fd705e858"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Display training curves using data from history",
   "id": "73c218883d8e4a7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We plot the curves using the iteration as the x-axis:\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df_train['Iteration'], df_train['Training Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(df_val['Iteration'], df_val['Validation Loss'], label='Val Loss', color='orange')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df_train['Iteration'], df_train['Training Accuracy'], label='Train Acc', color='green')\n",
    "plt.plot(df_val['Iteration'], df_val['Validation Accuracy'], label='Val Acc', color='red')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(experimentName, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outputFolder, 'training_curves.png'))\n",
    "plt.show()"
   ],
   "id": "3a9d581b8d9e2d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2) Compute metrics on the test dataset",
   "id": "2009bfe2b22ae4f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Q16.** Eval the model on the test dataset\n",
    "\n",
    "Your code should store the predicted class number in the list `allYtest` and the actual true values in the list `allYtrue`"
   ],
   "id": "4a8c06ce0310d157"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "allYtest = []\n",
    "allYtrue = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Send to gpu\n",
    "        images = images.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Get back the predicted classes\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update lists\n",
    "        allYtest.extend(predicted.cpu().numpy())\n",
    "        allYtrue.extend(labels)"
   ],
   "id": "eb5d51761046df25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Display & save confusion matrix over the test dataset\n",
    "\n",
    "Use the `classification_report` to display some metrics and confusion_matrix along with sb.heatmap to display the confusion matrix.\n",
    "(sklearn confusion_matrix & classification report are already imported, cf beginning of the program)\n",
    "\n",
    "Use Matplotlib to save the file in `os.path.join(outputFolder,'confusion_matrix_test.png')`"
   ],
   "id": "c05ee994fad39bfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(classification_report(allYtrue, allYtest))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(confusion_matrix(allYtrue, allYtest), annot=True, xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.suptitle(experimentName, fontsize=16)\n",
    "plt.savefig(os.path.join(outputFolder, 'confusion_matrix_test.png'))"
   ],
   "id": "5d139b6c6344c9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following code save some data in \"model_result.txt\" a file with the result and the model structure for later analysis",
   "id": "b4c72bedc0e95ba2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelStr = str(summary(model, input_size=[batchSize, 3, H, W]))\n",
    "classificationReport = classification_report(allYtrue, allYtest)\n",
    "\n",
    "with open(os.path.join(outputFolder, \"model_result.txt\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    # write hyperparameters\n",
    "    file.write(f\"Parameter for run : {experimentName} \\n\")\n",
    "    file.write(f\"Training duration : {trainingDuration:.2e} s \\n\")\n",
    "    file.write(f\"Batch size   : {batchSize} \\n\")\n",
    "    file.write(f\"Image size   : ({H},{W})\\n\")\n",
    "    file.write(f\"learningRate : {learningRate:e} \\n\")\n",
    "    file.write(f\"numEpoch     : {numEpoch} \\n\")\n",
    "    file.write(f\"weight_decay : {weight_decay} \\n\")\n",
    "    file.write(\"\\n\\n\\n\")\n",
    "    file.write(classificationReport)\n",
    "    file.write(\"\\n\\n\\n\")\n",
    "    file.write(modelStr)"
   ],
   "id": "f877bb9a930c72c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5) Classify some images",
   "id": "47d7bb045e45c729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "imgs, yTrue = next(iter(test_loader))\n",
    "imgs = imgs.to(device)\n",
    "logits = model(imgs)\n",
    "yPred = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
    "correct = yPred.to('cpu') == yTrue\n",
    "imgs = imgs.to('cpu')\n",
    "\n",
    "nImgs = 30\n",
    "cols = int(math.ceil(math.sqrt(nImgs)))     # Number of columns (square root rounded up)\n",
    "rows = int(math.ceil(nImgs / cols))         # Number of rows (ensure enough rows to fit images)\n",
    "\n",
    "# Create the figure with the calculated number of rows and columns\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))  # Adjust figsize to the grid\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(nImgs):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(imgs[i, :].permute(1, 2, 0))\n",
    "    title = \"Correct\" if correct[i].item() else \"Incorrect\"  # Title based on the result\n",
    "    ax.set_title(title, fontsize=10, color=\"green\" if correct[i].item() else \"red\")  # Color-code titles\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(experimentName, fontsize=16)\n",
    "plt.savefig(os.path.join(outputFolder, \"sample_result.png\"))"
   ],
   "id": "ea9e21c639703ca7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save data",
   "id": "96a40b9e5424a49e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def zip_compression_tree(folderIn, zip_name, ):\n",
    "    with zipfile.ZipFile(zip_name, 'w') as z:\n",
    "        for root, dirs, files in os.walk(folderIn):\n",
    "            folderOut = root.split(os.path.commonpath([folderIn, root]))[1]\n",
    "            for file in files:\n",
    "                z.write(os.path.join(root, file), arcname=os.path.join(folderIn, folderOut, file))"
   ],
   "id": "f9ad9dc3a775c825"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To be run when all steps are done",
   "id": "963cfbb97228c29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# resultZipFilename = \"result.zip\"\n",
    "# zip_compression_tree(outputFolder, resultZipFilename)"
   ],
   "id": "215fcea885f6242d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#from google.colab import files\n",
    "#files.download(resultZipFilename)"
   ],
   "id": "ed5ecc3b4c512036"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
