{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# The benefits of exploring randomly the parameter space of a model",
   "id": "7df0320c9c1026c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Perceptron Parameter Search Space\n",
    "\n",
    "We will explore the search space of parameters of a simple perceptron and plot the loss against the perceptron's weight and bias.\n",
    "\n",
    "We will create a simple dataset, define a perceptron model, compute the loss for different parameter values, and visualize the results."
   ],
   "id": "fa3dcd41be8acb0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "\n",
    "# Create a simple dataset\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand(100, 1)  # 100 data points with a single feature\n",
    "y = 3 * X + 1 + 0.1 * torch.randn(100, 1)  # Linear relationship with some noise\n",
    "y = torch.div(y - y.min(), y.max() - y.min())  # Bound y to 0 and 1\n",
    "\n",
    "# Define a simple perceptron model\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplePerceptron, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Mean squared error loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Uniform exploration of parameter space\n",
    "def uniform_exploration(weights, biases):\n",
    "    start_time = time.time()\n",
    "    loss_values = []\n",
    "\n",
    "    for w in weights:\n",
    "        for b in biases:\n",
    "            model = SimplePerceptron()\n",
    "            model.linear.weight.data.fill_(w)\n",
    "            model.linear.bias.data.fill_(b)\n",
    "            loss = criterion(model(X), y)\n",
    "            loss_values.append([w, b, loss.item()])\n",
    "\n",
    "    loss_values = np.array(loss_values)\n",
    "    min_loss = np.min(loss_values[:, 2])\n",
    "    idx_min_loss = np.argmin(loss_values[:, 2])\n",
    "    best_w, best_b = loss_values[idx_min_loss, 0], loss_values[idx_min_loss, 1]\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    return loss_values, best_w, best_b, min_loss, execution_time\n",
    "\n",
    "# Gradient descent optimization\n",
    "def gradient_descent(model, optimizer, n_epochs):\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "    return model, losses, execution_time\n",
    "\n",
    "# Define parameter ranges\n",
    "weights = np.linspace(0, 1, 100)\n",
    "biases = np.linspace(0, 1, 100)\n",
    "\n",
    "# Uniform exploration\n",
    "loss_values, best_w, best_b, min_loss, uniform_time = uniform_exploration(weights, biases)\n",
    "\n",
    "# Plot the 3D search space\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(loss_values[:, 0], loss_values[:, 1], loss_values[:, 2], c=loss_values[:, 2], cmap='viridis')\n",
    "ax.set_xlabel('Weight (w1)')\n",
    "ax.set_ylabel('Bias (x2)')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.set_title('Loss Landscape of a Simple Perceptron')\n",
    "plt.show()\n",
    "\n",
    "print(f'Uniform Exploration - Best weight: {best_w}, Best bias: {best_b}, Minimum loss: {min_loss}, Time: {uniform_time:.4f} seconds')\n",
    "\n",
    "# Best model from uniform exploration\n",
    "best_model = SimplePerceptron()\n",
    "best_model.linear.weight.data.fill_(best_w)\n",
    "best_model.linear.bias.data.fill_(best_b)\n",
    "\n",
    "# Plot the best fit line from uniform exploration\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, best_model(X).detach().numpy(), color='red', linewidth=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Best Fit Line (Uniform Exploration)')\n",
    "plt.show()\n",
    "\n",
    "# Gradient descent\n",
    "model = SimplePerceptron()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "model, losses, grad_time = gradient_descent(model, optimizer, n_epochs=1000)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Loss Curve (Gradient Descent)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Gradient Descent - Best weight: {model.linear.weight.item()}, Best bias: {model.linear.bias.item()}, Minimum loss: {losses[-1]}, Time: {grad_time:.4f} seconds')\n",
    "\n",
    "# Plot the best fit line from gradient descent\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model(X).detach().numpy(), color='red', linewidth=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Best Fit Line (Gradient Descent)')\n",
    "plt.show()\n"
   ],
   "id": "94921b022b8a79a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neural Network Parameter Search Space on MNIST Dataset\n",
    "\n",
    "In this notebook, we will explore the search space of parameters of a neural network using the MNIST dataset. We will compare uniform exploration and gradient descent methods."
   ],
   "id": "ad20270a02152d3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import tqdm"
   ],
   "id": "b06e8341c6eb3144"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100000, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Cross-entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Uniform exploration of parameter space\n",
    "def uniform_exploration(n_samples):\n",
    "    start_time = time.time()\n",
    "    loss_values = []\n",
    "\n",
    "    models = []\n",
    "    for _ in tqdm.tqdm(range(n_samples)):\n",
    "        model = SimpleNN()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param.uniform_(-1, 1)\n",
    "            model.fc1.weight.data.uniform_(-1, 1)\n",
    "            model.fc1.bias.data.uniform_(-1, 1)\n",
    "            model.fc2.weight.data.uniform_(-1, 1)\n",
    "            model.fc2.bias.data.uniform_(-1, 1)\n",
    "            model.fc3.weight.data.uniform_(-1, 1)\n",
    "            model.fc3.bias.data.uniform_(-1, 1)\n",
    "        models.append(model)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_values.append([model.fc1.weight.mean().item(), model.fc1.bias.mean().item(), loss.item()])\n",
    "\n",
    "    loss_values = np.array(loss_values)\n",
    "    min_loss = np.min(loss_values[:, 2])\n",
    "    idx_min_loss = np.argmin(loss_values[:, 2])\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    return loss_values, min_loss, execution_time, models[idx_min_loss]\n",
    "\n",
    "# Gradient descent optimization\n",
    "def gradient_descent(model, optimizer, n_epochs):\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    return model, losses, execution_time"
   ],
   "id": "677887a8e9ca0647"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Uniform exploration\n",
    "loss_values, min_loss, uniform_time, best_model = uniform_exploration(n_samples=10)\n",
    "print(f'Uniform Exploration - Minimum loss: {min_loss}, Time: {uniform_time:.4f} seconds')"
   ],
   "id": "f212ff07bc8850b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Gradient descent\n",
    "model = SimpleNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "model, losses, grad_time = gradient_descent(model, optimizer, n_epochs=5)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Loss Curve (Gradient Descent)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Gradient Descent - Minimum loss: {losses[-1]}, Time: {grad_time:.4f} seconds')"
   ],
   "id": "c54d8894ff5ed59d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use PCA for dimensionality reduction and visualization of multi-dimensional inputs\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(loss_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=loss_values[:, 2], cmap='viridis')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA of Loss Landscape')\n",
    "plt.colorbar(label='Loss')\n",
    "plt.show()\n",
    "\n",
    "# Use t-SNE for dimensionality reduction and visualization of multi-dimensional inputs\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(loss_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=loss_values[:, 2], cmap='viridis')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE of Loss Landscape')\n",
    "plt.colorbar(label='Loss')\n",
    "plt.show()"
   ],
   "id": "462a87d49252dbb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compare outputs between the best model from uniform exploration and gradient descent\n",
    "def compare_outputs(model1, model2):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    images, labels = next(iter(train_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs1 = model1(images)\n",
    "    outputs2 = model2(images)\n",
    "    _, predicted1 = torch.max(outputs1, 1)\n",
    "    _, predicted2 = torch.max(outputs2, 1)\n",
    "    return images, predicted1, predicted2\n",
    "\n",
    "images, predicted_uniform, predicted_grad = compare_outputs(best_model, model)"
   ],
   "id": "c8d328fcf68a047d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dedd7dc3bb77b3d"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
